When people refer to the size of a model, they are usually talking about the number of parameters it has. 
Parameters are the internal variables that the model learns from the training data. 
The more parameters a model has, the more complex patterns it can capture, but it also requires more computational resources.

For example, when someone mentions a "7 billion" model, they are likely referring to a model with 7 billion parameters. 
Larger models tend to perform better on complex tasks but may also require more powerful hardware for training and inference.